---
title: "Agrégation géographique avec contrainte de contiguïté"
author: "Rym Boulassel, Pierre Le Maux , Nathan Randriamanana"
date: "2023-02-21"
output: html_document
---

Le code qui suit présente une proposition d'implémentation d'une agrégation géographique avec contrainte de contiguïté prenant en compte un critère d'arrêt sur la taille du nombre de répondants d'une enquête pour chaque agrégat, lors du processus d'agrégation.

Ce code pouvant très bien s'adapter sur des données réelles est effectué sur des données simulées pour éprouver les possibilités de notre approche. La forme des données simulées permet de saisir le format attendu sur les données réelles pour une éventuelle application. Ce projet ne propose pas une mise en production mais une proposition méthodologique de l'implémentation d'une telle méthode.

## Import des packages

```{r setup, results = FALSE}
library(ggplot2) # package pour affichage graphique de la grille
library(dplyr) # package pour manipuler les données
library(igraph) # package pour la manipulation de graphe
# package pour manipulation avancée de dendrogramme
if(!require(dendextend)) install.packages("dendextend")
library("dendextend")
```

# Simulation des communes : génération de carrés aléatoires

## Paramétrage de la grille de carrés

```{r}
#dimension de l'espace géographique simulé
x_max <- y_max <- 20
# effectif de la population d'individus à générer
n <- 500 
# nombre total de carrés dans la grille
n_squares <- 25 
# nombre de colonnes de carrés
nb_cols_squares <- sqrt(n_squares) 
square_side_length <- x_max/nb_cols_squares # longueur des côtés d'un carré
```

## Génération des individus et de la grille

### Génération des individus et données de contexte

On génère d'abord des individus aléatoirement dans le plan et générant au hasard des couples (x,y)
représentant les individus.

Pour pouvoir observer les mêmes individus à chaque fois que l'on souhaite recommencer la simulation,
on fixe ce qu'on appelle une graine d'aléatoire.

```{r}
set.seed(123) # fixer la graine d'aléatoire pour obtenir les mêmes résultats aléatoires à chaque exécution
```

Nous générons ensuite nos individus selon une loi uniforme dont les données sont gardées dans un tableau.

```{r}
# génération aléatoire d'individus dans le plan
df <- data.frame(x = runif(n, 0, x_max),
                 y = runif(n, 0, y_max))

```

Nous générons les données de contexte des communes i.e une valeur aléatoire est affectée à chaque commune (carré) de notre simulation

```{r}
# génération des données de contexte
square_values <- runif(n_squares, 0, 1) # valeurs aléatoires pour chaque carré
```

Bien que cela ne soit pas utile dans notre étude, nous pouvons générer une valeur aléatoire pour chaque individu (point) qui représente une donnée individuelle pouvant être une donnée d'enquête

```{r}
#générer une valeur aléatoire pour chaque point (individu d'enquête) dans les coordonnées
df$survey_value <- runif(n, 0, 1)
```

```{r}
#résumer et afficher un tableau de synthèse de ces données
summary(df)
```

### Génération de la grille 

Nous découpons le plan en fonction de la longueur des côtés de chaque carré et la dimension du plan.
Il faut donc dans la partie "Paramétrage de la grille de carrés", choisir des dimensions cohérentes à la situation que l'on souhaite modéliser.

```{r}
#découper les coordonnées en carrés de taille square_side_lengthxsquare_side_length
#la fonction cut sert à découper les coordonnées en intervalles de taille prédéterminée
#la fonction seq sert à générer une séquence d'éléments dans un ordre donné
df$x_bin <- cut(df$x, breaks = seq(0, x_max, by = square_side_length), labels = FALSE)
df$y_bin <- cut(df$y, breaks = seq(0,y_max, by = square_side_length), labels = FALSE)

# Assignation d'un numéro ou indice à chaque carré. Ex: carré n°1 est le carré le plus haut à gauche
# en numérotant les carrés, l'identifiant de chaque carré s'obtient à partir des coordonnées x_bin et y_bin (expression modulo)
df$square_id <- df$x_bin + nb_cols_squares * (df$y_bin-1) #petite réflexion mathématique
#ex: le carré n°6 se déduit avec x_bin=1,y_bin=2 par 6=1+2(5) on se déplace de 1 à droite et sachant qu'il faut se deplacer de 5 pas à droite avant d'arriver en bas, on s'est déplacé 2 cases en bas.
```

Nous raisonnons aussi sur les numéros que l'on donne aux communes. La convention que nous avons choisie est de numéroter les carrés de gauche à droite et de haut en bas.

Exemple pour une grille de 25 carrés de dimension 4x4 dans un plan de dimension 20x20:
![unchanged image](Templates/survey_data_square_with_ID.png)

Pour un individu, x_bin correspond à la colonne de carré correspondante.
y_bin correspond à la ligne de carré correspondante.

Par exemple, un individu à la position (x=6,y=7) se trouve dans la deuxième colonne de carré (x_bin=2) car un carré a un côté de longueur 4 et dans la deuxième ligne de carré (y_bin=2). En observant la grille, cela correspond au carré n°7.

![unchanged image](Templates/Screenshot 2023-02-21 at 17-02-20 RStudio Server.png)

Chaque carré ayant un identifiant, nous pouvons lui affecter une donnée de contexte.

```{r}
#Ajout de la colonne correspondant à la valeur de chaque carré
df$square_value <- square_values[df$square_id]
```

La grille est donc bien modélisée selon le tableau suivant :

```{r}
head(df)
```

Voici à quoi pourrait typiquement ressembler les données sur lesquelles on souhaiterait appliquer une agrégation géographique avec contrainte de contiguïté.

# Représentation graphique de notre grille

```{r ,echo=FALSE,include=FALSE} 
p_numerote <- ggplot(df, aes(x, y)) +
  geom_bin2d(bins = n_squares) +
  coord_fixed(ratio = 1) +
  labs(x = "", y = "") +
  theme_classic() +
  scale_y_reverse() +
  theme(plot.background = element_rect(fill = "white"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "black",size = square_side_length/x_max,linetype = 1),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        plot.margin = unit(c(0, 0, 0, 0), "cm")) +
  # numérotation des carrés
  geom_text(aes(x = (x_bin - 0.5) * square_side_length, 
                y = (y_bin - 0.5) * square_side_length,
                label = square_id, size=6), 
            color = "red", 
            size = 5,
            family = "sans",
            hjust = 0.5,
            vjust = 0.5)

# Enregistrement du graphique en tant qu'image
ggsave("survey_data_square_with_ID.png", plot = p_numerote, width = n_squares, height = n_squares,limitsize = FALSE)
#Affichage de la grille
p_numerote
```

# Mise en forme du tableau : Traitement des données simulées pour être fournies en entrée à notre algorithme d'agrégation

Nous renommons et filtrons les variables pertinentes pour une meilleure compréhension dans le contexte du problème. Nous n'avons besoin que des données des contexte et les identifiants des communes. Il nous reste le nombre de répondants à l'enquête par commune.

```{r}
#Restitution du tableau final qui va être utilisé
df$coords <- paste(df$x, df$y, sep = ",")
#Filtrer les données et renommer les colonnes
donnees <- dplyr::select(df,square_value,square_id)
colnames(donnees) <- c("donnee_contexte_commune", "square_ID")
```

Pour avoir le nombre de répondants à l'enquête dans chaque commune, nous agrégons les données individuelles à l'échelle des communes.

```{r, echo = FALSE, results='asis', comment=NA, warning=FALSE, message=FALSE}
# Fonction pour agréger les données individuelles et avoir le nombre de répondants par commune 
agg_indiv_to_communes <- function(data)
{data %>% group_by(square_ID,donnee_contexte_commune) %>% summarize(nb_repondants_commune = n())
}
donnees_agg <- agg_indiv_to_communes(donnees)
donnees_agg <- donnees_agg %>%rename(ID=square_ID)
```

Les données sont fin prêtes pour l'agrégation.

```{r}
head(donnees_agg)
```

# Codification de la structure de voisinage

Il est important de définir la notion de voisinage car elle diffère selon le type que l'on a choisie.
Les deux types de voisinages les plus usuels sont la contiguïté "QUEEN" et "ROOK", analogue au possibilité de déplacement des pièces correspondantes aux jeux d'échecs.

![unchanged image](Templates/contiguite.png)

En pratique, il faut renseigner les voisins à la main. Dans le cadre d'une grille de commune, il est possible d'écrire un algorithme pour définir une structure de voisinage en raisonnant sur les identifiants des communes.
En utilisant des considérations arithmétiques et en traitant les cas particuliers (carrés sur les bords gauche et droite), il est possible d'écrire cela pour notre grille 25x25.


## Création de la matrice de contiguïté

Nous créons deux fonctions codant les deux différents types de voisinage. Pour la suite, nous appliquerons la contiguité ROOK i.e il ne suffit pas qu'elles partagent un point de frontière commune pour que deux communes soient voisines.

Choisir le type de contiguïté revient de manière équivalente à choisir une certaine distance à appliquer entre deux indices de carré dans une grille (ou un damier) et de ne garder que les voisins directs i.e qui n'ont qu'une distance de 1 avec le carré considéré.

Exemple: avec la distance de Tchebychev, les voisins directs du roi ont une distance de 1 et on ne retient qu ceux-là. La distance de Manhattan donnent les mêmes résultats mais sans les diagonales. Nous implémentons donc ces distances pour créer notre matrice de contiguïté.

![unchanged image](Templates/chebychev.png)

Les fonctions implémentées suivantes s'inspirent des distances de Manhattan et de Chebychev entre deux carrés du damier dont les indices sont i et j, sans tenir compte de leur position réelle dans l'espace.

Si l'on souhaite expérimenter d'autres distances plus exotiques, il est possible de les implémenter ci-dessous et de les rajouter dans le code qui suit.

```{r}
distance_tchebychev_indices=function(i,j){return(max(abs((j-1) %/% n - (i-1) %/% n), abs((j-1) %% n - (i-1) %% n))) }
distance_manhattan_indices=function(i,j){return(abs((j-1) %/% n - (i-1) %/% n) + abs((j-1) %% n - (i-1) %% n)) }
```



```{r}
create_contig_matrix = function(n, type="Rook"){
a <- b <- c()
for (i in 1:(n^2)) {
  for (j in 1:(n^2)) {
    ifelse(type=="Rook",diff <- distance_manhattan_indices(i,j),ifelse(type=="Queen",diff <- distance_tchebychev_indices(i,j),"Précisez un type de contiguïté existant: Rook ou Queen") )
    # la condition diff > 1 permet de filtrer les paires de carrés qui ne sont pas des voisins directs
    # la condition i >= j permet d'éviter les doublo
    if (diff > 1 || i >= j) {
      next
    } else {
      a <- c(a, i)
      b <- c(b, j)
    }
  }
}
voisin <- cbind(a, b)
colnames(voisin) <- c("id1","id2")
voisin <- data.frame(voisin)
#conversion des colonnes en chaine de caracteres
voisin$id1 <- as.character(voisin$id1)
voisin$id2 <- as.character(voisin$id2)
#conversion en tibble pour les fonctions d'aggrégation
contig_matrix <- as.matrix(voisin)
return(contig_matrix)
}
```

Nous pouvons enfin créer notre matrice de contiguïté. Nous renseignons en argument de la fonction le nombre de carrés que l'on a sur une rangée (ligne ou colonne)

```{r}
contig_matrix <- create_contig_matrix(nb_cols_squares)
```


Si l'on préfère la contiguité Queen, on peut rajouter l'argument "Queen" comme ci-dessous :

```{r,results = FALSE}
create_contig_matrix(sqrt(nb_cols_squares),"Queen")
```

# Mise en place de l'agrégation

## Clustering avec contraintes de contiguïté et critère d'arrêt sur les répondants

Nous définissons différentes fonctions utilitaires pour la gestion des contraintes.

### Fonction de contiguïté

Nous créons une fonction qui vérifie si 2 agrégats i et j sont géographiquement voisins selon la matrice de contiguïté que nous avons définie. Chaque agrégat contient plusieurs communes. La convention que nous avons choisie pour dire que 2 agrégats sont voisins est de regarder si au moins une commune d'un agrégat est voisine au minimum avec une commune de l'autre agrégat.

```{r}
groupes_voisins <- function(groups,i,j){ #fonction qui évalue si les agrégats i et j sont voisins
  res=F
  for (k in 1:length(contig_matrix[,1])){
    for (x in groups[[i]]){
      for (y in groups[[j]]){
        if((contig_matrix[k,"id1"]==x && contig_matrix[k,"id2"]==y)|(contig_matrix[k,"id1"]==y && contig_matrix[k,"id2"]==x)){
          res=T
        }
      }
    }
  }
  return(res)
}
```

### Fonction de distance entre les agrégats de commune

Plutôt que de définir seulement une fonction de distance entre deux communes, nous définissons une mesure de similarité entre 2 agrégats. Nous considérons que deux communes correspondent à deux agrégats qui contiennent respectivement chacun une unique commune.

Nous souhaitons regrouper les communes qui se ressemblent le plus en terme de données de contexte. Les communes doivent donc être les plus homogènes possibles au sein d'un agrégat et les agrégats doivent être les plus différents possibles. En conséquence, la fonction est donc construite sur les données de contexte associées à chaque commune.

Pour cela, nous choisissons une mesure de similarité très performante pour répondre à notre besoin, et classiquement utilisé pour le clustering hiérarchique (notamment populaire avec le langage R). Il s'agit de la distance de Ward.D2 dont l'implémentation est détaillée dans "Ward’s Hierarchical Clustering Method:Clustering Criterion and Agglomerative Algorithm", Pierre Legendre, Fionn Murtagh.

Pour expliquer brièvement le principe, elle mesure la variance totale de toutes les données à travers tous les agrégats créés par la classification. Elle est calculée en comparant la somme des carrés des distances de chaque point à son centre de gravité dans chaque agrégat. Cette distance mesure ainsi la différence entre la somme des carrés des distances de chaque objet dans le même agrégat et la somme des carrés des distances de chaque objet dans des agrégats différents.

```{r}
distance_between_groups <- function(groups,i,j) {
  # Calcul de la distance entre deux groupes pour la méthode ward.d2
  # voir "Ward’s Hierarchical Clustering Method:Clustering Criterion and Agglomerative Algorithm"
  n <- length(groups[[i]]) + length(groups[[j]])
  # Filtre des valeurs de la variable d'intérêt associés aux ID des clusters 
  data_group1 <- filter(donnees_agg,ID %in% groups[[i]])$donnee_contexte_commune
  data_group2 <- filter(donnees_agg,ID %in% groups[[j]])$donnee_contexte_commune
  mean1 <- mean(data_group1)
  mean2 <- mean(data_group2)
  mean_all <- mean(c(mean(data_group1), mean(data_group2)))
  # Calcul de la somme des carrés des écarts à la moyenne pour chaque groupe
  ssd1 <- sum((data_group1 - mean1) ^ 2)
  ssd2 <- sum((data_group2 - mean2) ^ 2)
  # Calcul de la somme des carrés des écarts à la moyenne pour tous les points
  ssd_all <- sum((c(data_group1,data_group2) - mean_all) ^ 2)
  # Calcul de la distance entre deux groupes pour la méthode ward.d2
  dist_between_groups <- ssd_all - (ssd1 + ssd2)
  return(sqrt(dist_between_groups / (n)))
}
```

La mesure de similarité précédente prend en compte des considérations purement statistiques mais comme nous nous trouvons dans le cadre d'un thème précis qui est la classification **géographique**, il est possible voire davantage cohérent de tirer profit d'une pondération des communes. 

Tout comme, il est proposé dans le code proposé par M. Hisnard, "l’utilisateur est évidemment libre de choisir toute autre distance. Il convient de remarquer que le choix de la distance peut influer sur la taille des classe créées, par-là sur le nombre de voisins à chaque itération et donc sur la durée du programme."

Plutôt que la distance de Ward.D2 classique, il peut être intéressant de regarder celle entre deux unités dans un ensemble de données, en prenant en compte une variable pondérée nommée "Poids". ( explication d'ajout du terme P1*P2/(P1+P2))

### Fonction de fusion des agrégats.

Comme dans la plupart des packages dédiée au clustering hiérarhique, nous représentons notre partition par une liste de vecteurs. Chaque vecteur contient les identifiants des communes (ex: "1", "2" dans notre cas). Un agrégat est donc modélisé par un vecteur.
Nous fusionnons donc 2 agrégats en concaténant les identifiants de communes de chaque agrégat dans un nouveau vecteur qui représente le nouvel agrégat et nous supprimons les 2 anciens vecteurs représentant les anciens agrégats.

```{r}
fusion_between_groups <- function(i,j) {
  #fusion des groupes
  groups <- c(groups, list(c(groups[[i]],groups[[j]])))
  #suppression des anciens groupes ayant servi à la fusion
  groups <- groups[-c(i, j)]
}
```

### Fonction du calcul d'inertie inter-intra au niveau des donnée de contexte.

Pour avoir la partition la plus homogène possible au sein de chaque classe et hétérogène entre les classes, nous définissons une fonction qui calcule les inertie inter et intra-classes d'une partition.

```{r}
inertie_inter_intra_donnee_contexte_commune <- function(data, groups) {
  K <- length(groups)
  N <- nrow(data)
  total_mean <- mean(data$donnee_contexte_commune)
  
  inter_group_var <- 0
  for (i in seq_len(K)) {
    group_mean <- mean(filter(data, ID %in% groups[[i]])$donnee_contexte_commune)
    group_size <- nrow(filter(data, ID %in% groups[[i]]))
    inter_group_var <- inter_group_var + group_size * (group_mean - total_mean)^2
  }
  
  intra_group_var <- sum((data$donnee_contexte_commune - total_mean)^2)
  
  return(list(inter = inter_group_var, intra = intra_group_var))
}
```

# Initialisation de l'agrégation avec critère d'arrêt sur le nombre de répondants.

## Paramétrage de l'algorithme de clustering

Le principe d'une CAH est de regrouper par paire les voisins qui se ressemblent le plus. Habituellement, il est courant d'arrêter le clustering lorsqu'on obtient plus qu'un seul cluster. Le statisticien choisit ensuite le nombre de clusters en fonction des sorties du dendrogramme et à l'aide d'autres métriques.

Nous allons opérer un peu différemment. Notre objectif est de continuer à regrouper des communes dans un agrégat jusqu'à ce que ce même agrégat atteint un nombre de répondants à l'enquête suffisant soit 100 (choix arbitraire pour commencer). Par conséquent, le partionnement sera fin et il ne sera souvent ni utile ni possible d'obtenir un seul cluster à la fin du processus d'agrégation à cause du critère d'arrêt sur les répondants. Pour que notre algorithme converge, nous optons donc pour une condition de stabilisation de l'inertie, en l'occurence le rapport inertie inter-intra classe.

```{r}
#Nombre maximum de répondants par agrégats
nb_repondants_max <- 100
#Fixation de l'écart absolu souhaité pour la stabilisation de l'inertie entre l'étape précédente et actuelle
seuil_inertie <- 0.000001
#Fixation arbitraire de l'inertie précédente pour amorcer l'algorithme
inertie_prec = 100000
#Fixation de l'état de convergence à FALSE tant que le processus d'agrégation ne se termine pas
convergence = FALSE
```

## Initialisation de la partition

Comme nous avons préféré travailler directement sur une mesure de similarité entre les groupes au lieu d'une fonction de distance entre les communes pour des facilités de généralisation et d'implémentation, nous initialisons la partition en considérant qu'au départ chaque commune est son propre agrégat. L'agrégat 1 comporte la commune "1", ..., l'agrégat n_squares comporte la commune "n_squares". Nous pourrons ainsi directement fusionner les agrégats qui se ressemblent sous contraintes. Nous attribuons aussi un attribut à chaque commune-agrégat pour afficher le nombre de répondants présent dans chaque. 

L'avantage est que nous n'avons plus maintenant qu'à raisonner sur les agrégats pour la fusion sans se soucier des communes, le travail étant fait.

```{r}
# Initialisation des groupes
groups = lapply(1:n_squares, function(x) c(as.character(x)))
# stockage du nombre de répondants
for (i in 1:length(groups)){attr(groups[[i]], "nombres_répondants") <- filter(donnees_agg,ID %in% groups[[i]])$nb_repondants_commune}
for (i in 1:length(groups)){attr(groups[[i]], "height") <- 0}
head(groups)
```

Nous initialisons une matrice de distance entre les agrégats de dimension n_squares*n_squares. La taille de la matrice va diminuer au fur et à mesure que le nombre d'agrégats diminue.

À l'initialisation, la matrice ne renseignera la distance entre 2 communes qui sont voisines et dont la somme de nombre de répondants n'excède pas le seuil de répondants fixé. Une valeur Infinie est donnée pour les autres paires de communes pour éviter d'avoir un minimum de distance en dehors des contraintes de contiguïté et de nombre de répondants puisque de toute façon seules les communes voisines entre-elles nous intéressent. L'idée de rajouter des termes infinis s'inspirent du papier "Hierarchical Clustering with Contiguity Constraint in R" rédigé par Pierre Legendre et Guillaume Guénard.

```{r}
# Initialisation de la matrice de distance entre les groupes
dist_mat <- matrix(Inf, nrow = length(groups), ncol = length(groups))
for (i in 1:length(groups)) {
  for (j in i:length(groups)) {
    if (j>i){
      #dès la première étape,on exclut d'office du calcul les groupes trop éloignés et trop gros: calcul plus rapide
      #contrainte contiguite
      if (groupes_voisins(groups,i,j)){ 
        #contrainte de taille sur le nombre de répondants par agrégat
        if (sum(filter(donnees_agg,ID %in% groups[[i]])$nb_repondants_commune)
            + sum(filter(donnees_agg,ID %in% groups[[j]])$nb_repondants_commune) <= nb_repondants_max) { #contrainte de taille sur nombre de répondants
          dist_mat[i,j] <- distance_between_groups(groups,i,j)
          dist_mat[j,i] <- dist_mat[i,j]
        }
      }
    }
    
  }
}
```

## Stockage des différentes inerties et des partitions

À chaque itération, nous allons garder en mémoire l'inertie et la partition associée afin qu'à la convergence de notre algorithme, nous sélectionnons la partition ayant le plus faible rapport d'inertie intra-inter classe
puisque nous voulons les agrégats les plus homogènes possibles en terme de données de contexte.

```{r}
# Initialisation de la liste des inerties inter-intra classes
liste_inertie <- c()
# Initialisation de la liste des listes des groupes pour sélectionner à la fin le groupe
#correspondant au minimum d'inertie inter-intra
liste_groups <- list()
```

## Boucle de l'agrégation géographique avec contrainte de contiguïté et critère d'arrêt sur les répondants à l'enquête

La boucle suivante itère et met à jour les éléments initialisés précédemment. À chaque étape, une matrice de similarité est calculé entre les agrégats. Les indices correspondant au premier élément minimum de la matrice sont stockés et correspondent aux agrégats i et j. 

Nous vérifions ensuite que les agrégats i et j vérifient les contraintes de contiguïté et si le critère d'arrêt n'est pas encore satisfait. Si on peut agréger les 2 agrégats, alors on met à jour la liste des partitions, la matrice de similarité ainsi que la liste des inerties.

```{r}
while (!convergence) {
  
  # Recherche des groupes les plus proches
  min_dist <- min(dist_mat)
  #tableau en 2 colonnes des couples d'indice du minimum
  min_indices <- which(dist_mat == min_dist, arr.ind = TRUE)
  i <- min_indices[1,][[1]]
  j <- min_indices[1,][[2]]
  
  #condition 
  if (groupes_voisins(groups,i,j)){ 
    #notion de voisinage entre 2 clusters à appliquer ici sur les vecteurs et non des unités
    # contient les indices de la première occurrence de la commune i dans chaque vecteur de la liste G
    nb_repondant_agregat <- sum(filter(donnees_agg,ID %in% groups[[i]])$nb_repondants_commune)
    + sum(filter(donnees_agg,ID %in% groups[[j]])$nb_repondants_commune)
    if ( nb_repondant_agregat <= nb_repondants_max) { #contrainte de taille sur nombre de répondants

      #ajouter calcul de distance et les voisins dans un des vecteurs de G.
      # Fusion des groupes
      new_group <- c(groups[[i]], groups[[j]])  
      groups[[i]] <- new_group
      
      # Stockage de la hauteur de la fusion
      height <- min_dist / 2
      
      # Stockage de la hauteur pour les deux groupes fusionnés
      attr(new_group, "height") <- height
      attr(groups[[i]], "height") <- height
      # Stockage du nombre de répondants pour les deux groupes fusionnés
      attr(new_group, "nombre_répondants") <- nb_repondant_agregat
      attr(groups[[i]], "nombres_répondants") <- nb_repondant_agregat
      cat("Nombre de répondants du cluster actuellement fusionné :",as.character(nb_repondant_agregat),"\n")
      
      # Suppression de l'ancien groupe
      groups <- groups[-j]
      # Suppression de ce cluster dans la matrice de distance des clusters
      dist_mat <- dist_mat[-j, -j]
      # Après cette suppression, i ne correpond plus au cluster designé auparavant réactualisons i
      i <- match(attr(new_group, "height"), sapply(groups, function(x) attr(x, "height")))
      
      #il peut être intéressant de stocker groups  à chaque étape
      #pour avoir un clustering proposé à chaque étape
      
      # Modification de la matrice de distance entre les groupes: nécessaire car les groupes changent 
      # On modifie la distance du nouveau cluster avec les autres existants
      # On recalcule la distance entre clusters à la ligne i et colonne i de l
      
      #le voisinage change donc aussi pour le nouveau cluster n°i fusionné
      
      for (k in 1:ncol(dist_mat)){
        #avant de recalculer les distances,on vérifie donc encore la contrainte de contiguité 
        #entre ce nouvel agrégat et les autres pour éviter les caluls inutiles
        if (groupes_voisins(groups,i,k)){
          dist_mat[i,k] <- dist_mat[k,i] <- distance_between_groups(groups,i,k)
        }
        else{dist_mat[i,k] <- dist_mat[k,i] <- Inf
        }
      }
      #par défaut, on fixe la diagonale à Inf pour ne pas 
      #associer une commune à elle-même
      dist_mat[i,i] <- dist_mat[i,i] <- Inf
      
        # Calcul de l'inertie inter-intra
        inertie_inter <- inertie_inter_intra_donnee_contexte_commune(donnees_agg, groups)$inter
        inertie_intra <- inertie_inter_intra_donnee_contexte_commune(donnees_agg, groups)$intra
        inertie = inertie_intra/inertie_inter
        # Sauvegarde du groupe et de l'inertie correspondante à chaque groupe formé/supprimé
        liste_inertie <- append(liste_inertie,inertie)
        liste_groups[[length(liste_groups) + 1]] <- groups
        }
    }
  # vérification de la convergence
  if(abs(inertie - inertie_prec) < seuil_inertie) {
    convergence = TRUE
    print("CONVERGENCE !!!")
  }
  cat("Inertie précédente:",inertie_prec,"Inertie suivante:",inertie,"\n")
  # mise à jour de la variable d'inertie précédente
  inertie_prec = inertie
}
```

Nous récupérons l'indice correspondant au minimum du rapport d'inertie intra-inter classes

```{r}
# On retourne la plus petite inertie inter-intra classes retenue pour pouvoir la comparer après
# Récupération de l'indice du minimum d'inertie et des groupes correspondants par la même occasion
# On prend la première occurence du minimum pour avoir trop de classes inutiles
min_inertie_donnee_contexte <- min(liste_inertie)
indice_min_groupes_retenu <- which(liste_inertie == min_inertie_donnee_contexte, arr.ind = TRUE)
```

À partir de cet indice, nous récupérons la partition correspondant à ce minimum et nous affichons les résultats souhaités

```{r}
# Résultats finaux
agregats_donnee_contexte_repondant <- liste_groups[[indice_min_groupes_retenu]]
print('Les agrégats au niveau des données de contexte retenus sur contrainte des répondants sont :')
print(agregats_donnee_contexte_repondant)
cat("Le rapport d'inertie obtenu sur les données de contexte correspondant vaut:", min_inertie_donnee_contexte,"\n")
```

# Affichage et mise en forme du résultat de l'agrégation

Nous présentons un pseudo-dendrogramme sommaire pour représenter les compositions des agrégats obtenus. Pour avoir un dendrogramme complet il est possible de transformer la partition obtenue en objet hclust et d'utiliser le package approprié mais le gain en terme de visualisation ne nous semblait pas suffisant par rapport au temps dont nous disposions.

```{r,echo=FALSE}
# Représentation dendrogramme à partir des hauteurs et des clusters (encore à développer)
# Conversion de dist_mat au bon format pour hclust en remplaçant les valeurs "Inf" par 65536
dist_mat_hclust <- dist_mat
dist_mat_hclust[is.infinite(dist_mat)] <- 100000000000000 # les valeurs infinies étant impossibles pour hclust
# Construire un objet de type dendrogramme
dend <- hclust(as.dist(dist_mat_hclust))
# Extraire les clusters de la liste et les rajouter en labels des clusters
labels(dend) <- lapply(agregats_donnee_contexte_repondant, function(x) x[1:length(x)])
# Afficher le dendrogramme
plot(dend)
```

Comme employé couramment dans les packages de clustering hiérarchique sur R, nous pouvons convertir la représentation de la partition par une liste de vecteurs en un simple vecteur où chaque indice correspond à la commune (ou carré)  et l'élément associé au numéro de l'agrégat auquel il appartient. Cela nous facilitera la tâche pour représenter graphiquement les agrégats obtenus en coloriant les carrés de la grille.

```{r}
# Résultat final pour représenter graphiquement sur les carrés (ne marche que dans notre cas particulier)
# Conversion de la liste de vecteurs en une liste résumant l'appartenance à un groupe
clusters <- lapply(agregats_donnee_contexte_repondant, as.numeric)
cluster_indices <- rep(NA, max(unlist(clusters)))

# Parcourir chaque vecteur numérique dans la liste "clusters"
for (i in seq_along(clusters)) {
  cluster_indices[clusters[[i]]] <- i
}
# Afficher la liste d'indices de cluster
clusters <- cluster_indices 
clusters
```

# Ajout du résultat de la classification au tableau de données 

Pour des possibilités de représentation à l'avenir avec d'autres outils, il peut être intéressant de rajouter une colonne aux données référant l'agrégat auquel chaque commune appartient suite à l'agrégation. Il est possible, par exemple, d'utiliser la plateforme de l'Observatoire des Territoires pour choisir un mode de visualisation cartographique des agrégats obtenus.

```{r}
#Appariement des identifiants des carrés aux clusters
donnees_agg$cluster <- clusters[donnees_agg$ID]
```

# Visualisation du résultat

```{r}
# Fonction pour assigner une couleur en fonction du numéro de carré
#Cette fonction attribue une couleur différente à chaque cluster en utilisant une palette de couleurs définie
palette <- c("red", "orange", "yellow", "green", "blue", "purple", "pink", "brown", "black", "gray",
               "darkred", "darkorange", "gold", "darkgreen", "navyblue", "indigo", "violet", "saddlebrown", "darkgray", "silver",
               "maroon", "tomato", "khaki", "olive", "skyblue", "slategray", "orchid", "sienna", "dimgray", "dimgrey",
               "crimson", "coral", "lemonchiffon", "limegreen", "cornflowerblue", "mediumorchid", "mediumslateblue", "sienna", "rosybrown", "lightslategray")

# Fonction pour assigner une couleur en fonction du numéro de carré
color_fun_2 <- function(id) {return(palette[id])}

# Assigner une couleur à chaque carré
color <- sapply(clusters, color_fun_2)
# Convertir au bon format pour l'affichage
color <- as.vector(as.matrix(color)[,1])

df <- data.frame(x_bin = rep(1:5, 5), 
                 y_bin = rep(5:1, each = 5), 
                 color = color)

ggplot(df, aes(x = x_bin, y = y_bin, fill = color)) +
  geom_rect(data = df, aes(xmin = x_bin - 0.5, xmax = x_bin + 0.5,
                           ymin = y_bin - 0.5, ymax = y_bin + 0.5),fill = df$color)+
  scale_fill_identity() +
  coord_cartesian(xlim = c(0, 6), ylim = c(0, 6)) +
  theme_void() +
  ggtitle("CAH sur les données de contexte avec contraintes de contiguité et sur le nombre de répondants")
```


